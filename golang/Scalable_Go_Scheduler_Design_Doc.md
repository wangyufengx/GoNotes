# 可扩展的 Go 调度程序设计文档[译]

​		本文假设你对[Go语言](https://go.dev/)和以及目前的 [Go 调度实现](http://code.google.com/p/go/source/browse/src/pkg/runtime/proc.c?r=01acf1dbe91f673f6308248b8f45ec0564b1d751)有一定了解。

>该文档是在写于2012年5月，该作者在第一代调度器的基础上设计出了这个新的调度器，然后通过该文档为大家阐述新的调度器模型及设计原理。

### 第一代调度器的问题

​		当前的 goroutine 调度器限制了用 Go 编写的并发程序的可扩展性，特别是高吞吐量服务器和并行计算程序。 Vtocc 服务器在 8 核机器上最大使用 70% CPU，而配置文件显示 14% 花费在 runtime.futex() 上。 通常，调度程序可能会禁止用户在性能至关重要的情况下使用惯用的细粒度并发。

#### 当前的实现有什么问题：

-   单一的全局互斥锁（Sched.Lock）和集中状态。

	互斥锁保护所有与 goroutine 相关的操作（创建、完成、重新调度等）。

-   Goroutine（G）切换（G.nextg）。

	工作线程 (M) 经常在彼此之间切换可运行的 goroutine，这可能会导致延迟增加和额外开销。每个 M 都必须能够执行任何可运行的 G，尤其是刚刚创建 G 的 M。

-   每个M的内存缓存（M.mcache）。

	内存缓存和其他缓存（堆栈分配的）跟所有的 M 有关，而其实并不需要这样。内存缓存其实仅需要跟正在运行 Go 代码的 M 关联（而无需跟阻塞在系统调用中的 M 关联）。正在运行 Go 代码的 M和所有 M 的比例高达 1:100，这样算下来，很明显，导致了过多无用的资源消耗（每个 Mcache 最多可以吃 2M 的内存）以及非常不好的作用域局部性。

-   频繁的线程阻塞/恢复。

	系统调用时，线程M经常被阻塞和恢复，从而增加了很多的开销。

### 设计

#### 处理器

​		总体思路是将 P（处理器）的概念引入运行时runtime，并在处理器之上实现窃取调度。
​		M 代表操作系统线程（就像现在一样）。 P 表示执行 Go 代码所需的资源。 当 M 执行 Go 代码时，它有一个关联的 P。当 M 空闲或在系统调用中时，就需要 P。
​		P 的数量有 GOMAXPROCS 个。 P 被设计组装在一个数组中，这样设计也是考虑到“窃取策略”的需要。 GOMAXPROCS 改变涉及停止/启动所有态以调整 P 数组的大小。
​		来自 sched 的一些变量被分散并移至 P。来自 M 的一些变量移至 P（与 Go 代码的主动执行相关的变量）。

```go
struct P
{
	Lock;
	G *gfree; // freelist, moved from sched
	G *ghead; // runnable, moved from sched
	G *gtail;
	MCache *mcache; // moved from M
	FixAlloc *stackalloc; // moved from M
	uint64 ncgocall;
	GCStats gcstats;
	// etc
	...
};

P *allp; // [GOMAXPROCS]
```

​		还有一个空闲 P 的无锁列表：

```
P *idlep; // lock-free list
```

​		当 M 想去执行 Go 代码时，必须先从上述的 P 列表中取出一个 P。当 M 结束执行 Go 代码时，它会将 P 放回到列表中。因此，当 M 执行 Go 代码时，它必然先关联 P。用这种机制代替来 sched.atomic (mcpu/mcpumax)策略。

#### 调度

​		当一个新的 G 被创建或一个现有的 G 变为可运行时，它会被放入当前 P 的可运行 goroutines 列表中。当 P 执行完 G 时，它首先尝试从自己的可运行 goroutines 列表中取出一个 G； 如果列表为空，P 选择一个随机受害者（另一个 P）并尝试从中窃取一半可运行的 goroutine。

#### Syscalls/M Parking and Unparking

​		当一个 M 创建一个新的 G 时，它必须确保有另一个 M 来执行 G（如果不是所有的 M 都已经忙了）。同样，当一个 M 进入 syscall 时，它必须确保有另一个 M 来执行 Go 代码。
​		有两种选择，我们可以立即阻止和解除阻止 M，也可以进行一些自旋。这是性能和消耗不必要的 CPU 周期之间的内在冲突。这里有个想法是使用自旋并消耗 CPU 周期。但是，它不会影响以 GOMAXPROCS=1 运行的程序（命令行实用程序、appengine 等）。
​		自旋是两级的：（1）空闲的 M 会关联 P 自旋寻找新的 G，（2）M w / o 关联的 P 自旋等待可用的 P。 最多有 GOMAXPROCS 个自旋的 M（（1）和（2））。 当存在类型（2）的空闲 M 时，类型（1）的空闲 M 不会被阻塞。
​		当产生一个新的 G 或者 M 进入系统调用时，或者 M 从空闲态转化为忙碌态时，它确保至少有 1 个自旋的 M（或所有 P 都处于繁忙状态）。 这样可以确保没有其他可以运行的可运行 G。 并避免同时进行过多的 M 阻塞/解除阻塞。
​		自旋通常是被动的（对 OS 的收益为 sched_yield（）），但可能包括一点主动的自旋（循环刻录 CPU）（需要调查和调整）。

#### 终止/死锁检测

​		终止/死锁检测在分布式系统中更成问题。 一般的想法是仅在所有 P 空闲时（空闲 P 的全局原子计数器）才进行检查，这允许进行涉及每个 P 状态聚合的更昂贵的检查。
尚无详情。

#### 锁操作系统线程

​		此功能不是性能关键。
1. 锁定的 G 变为不可运行（Gwaiting）。 M 立即将 P 放回到空闲列表，唤醒另一个 M 并阻塞。
2. 锁定 G 变为可运行（并到达 runq 的头部）。 当前 M 将自己的 P 和锁定的 G 移交给与锁定的 G 关联的 M，并解除对它的阻止。 当前 M 变为空闲。

#### 空闲 G

​		此功能不是性能关键。
​		有一个（或单个？）空闲 G 的全局队列。寻找工作的 M 在几次不成功的窃取尝试后检查队列。

### 实施计划

​		目标是将整个事情分成可以独立审查和提交的最小部分。

1. 引入P结构体（暂时为空）； 实现 allp/idlep 容器（idlep 是互斥保护的初学者）； 将 P 与运行 Go 代码的 M 关联起来。 全局互斥锁和原子状态仍然保留。

2. 将 G freelist 移动到 P。
3. 将 mcache 移动到 P。
4. 将 stackalloc 移动到 P。
5. 将 ncgocall/gcstats 移动到 P。
6. 去中心化运行队列，实现工作窃取。 消除 G 切换。 仍在全局互斥之下。
7、去除全局互斥锁，实现分布式终止检测，LockOSThread。
8. 实施旋转而不是提示阻塞/解除阻塞。
该计划可能会行不通，有很多未探索的细节。

### 潜在的进一步改进

1. 尝试 LIFO 调度，这将提高局部性。 然而，它仍然必须提供一定程度的公平性并优雅地处理 yielding goroutines。
2. 在goroutine第一次运行之前不要分配G和堆栈。 对于一个新创建的 goroutine，我们只需要 callerpc、fn、narg、nret 和 args，也就是大约 6 个单词。 这将允许创建大量运行到完成的 goroutine，内存开销显着降低。
3. G-to-P 更好的局部性。 尝试将未阻塞的 G 加入到它上次运行的 P 中。
4. P-to-M 更好的局部性。 尝试在上次运行的同一个 M 上执行 P。
6. M 创建的限制。 调度器很容易被迫每秒创建数千个 M，直到操作系统拒绝创建更多线程。 M 必须在 k*GOMAXPROCS 之前立即创建，之后可以通过计时器添加新 M。

### 散记

- GOMAXPROCS 不会因为这项工作而消失。

### 原文地址

https://docs.google.com/document/d/1TTj4T2JO42uD5ID9e89oa0sLKhJYD0Y_kqxDv3I3XMw/edit#heading=h.mmq8lm48qfcw

### 翻译参考地址

https://xie.infoq.cn/article/a6948402be688dba530094e9b
